{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidade Federal de Santa Catarina<br>\n",
    "Departamento de Engenharia Elétrica e Eletrônica<br>\n",
    "EEL7514/EEL7513 - Introdução ao Aprendizado de Máquina\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\ba}{\\mathbf{a}}$\n",
    "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
    "$\\newcommand{\\bA}{\\mathbf{A}}$\n",
    "$\\newcommand{\\bZ}{\\mathbf{Z}}$\n",
    "$\\newcommand{\\bS}{\\mathbf{S}}$\n",
    "$\\newcommand{\\bL}{\\mathbf{L}}$\n",
    "$\\newcommand{\\bR}{\\mathbf{R}}$\n",
    "$\\newcommand{\\bdelta}{\\boldsymbol{\\delta}}$\n",
    "$\\newcommand{\\bDelta}{\\boldsymbol{\\Delta}}$\n",
    "$\\newcommand{\\calD}{\\mathcal{D}}$\n",
    "$\\newcommand{\\mat}[1]{\\begin{bmatrix} #1 \\end{bmatrix}}$\n",
    "\n",
    "\n",
    "# Exercício 9: Redes Profundas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exercício, você irá explorar várias técnicas importantes para o treinamento de redes profundas:\n",
    "- Função de ativação ReLU\n",
    "- Variações do método do gradiente estocástico\n",
    "- Batch normalization\n",
    "- Dropout\n",
    "\n",
    "Lembre que, além da implementação, você deve incluir também uma **análise** dos resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruções gerais\n",
    "O exercício consiste em realizar as partes Q1, Q2 e Q3 do exercício:\n",
    "\n",
    "http://cs231n.github.io/assignments2018/assignment2/\n",
    "\n",
    "exceto a seção \"Layer Normalization\" da parte Q2.\n",
    "\n",
    "Obs: embora os slides de referência sejam do oferecimento de 2017, para o exercício é recomendável utilizar a versão mais atualizada, de 2018. Os slides de 2018 também podem ser consultados, ignorando o conteúdo novo sobre \"Layer Normalization\" (e, por hora, qualquer menção a redes convolucionais).\n",
    "\n",
    "## Instalação\n",
    "\n",
    "Ao rodar a primeira célula da Q1 do exercício, você observará alguns erros e warnings. Para corrigi-los, siga os seguintes passos:\n",
    "- Do arquivo `cs231n/gradient_check.py`, remova a linha \n",
    "```python\n",
    "from past.builtins import xrange\n",
    "```\n",
    "- Do arquivo `cs231n/solver.py`, remova as linhas \n",
    "```python\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()\n",
    "```\n",
    "- Ignore o warning \"_run the following from the cs231n directory and try again_\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observações\n",
    "\n",
    "A notação usada no exercício é frequentemente inconsistente com a notação usada nos slides da disciplina CS231n. Por exemplo, a matriz $\\bW$ aparece nos slides seguindo a notação que adotamos em sala, mas no exercício aparece transposta. É preciso ter cuidado para interpretar corretamente a orientação das matrizes, o que normalmente pode ser obtido analisando suas dimensões.\n",
    "\n",
    "Além disso, no exercício, todos os vetores são representados como vetores 1-D no NumPy, o que significa que são difundidos (broadcast) como vetores-linha. Vale a pena se acostumar com essa notação, pois ela de fato facilita a implementação.\n",
    "\n",
    "Várias funções por padrão utilizam a variável `x` para denotar a \"entrada\" da função, o que não necessiariamente significa um vetor de atributos.\n",
    "\n",
    "O termo _score_ é usado para denotar a variável $\\bz^{[L]}$, isto é, a saída da última camada antes da aplicação da função de ativação (no caso, softmax). A ideia por trás é que, em qualquer classificador binário ou um-contra-todos, a predição pode ser feita de acordo com o valor máximo do _score_, sem necessidade de aplicar a função de ativação. Assim, a função de ativação de saída só possui realmente utilidade para calcular a função perda, que por esse motivo é denominada \"perda softmax\" (na verdade, entropia cruzada categórica com saída softmax). Mencionamos esse fato para a ativação logística no slide 3 da Aula 6.\n",
    "\n",
    "Ao contrário dos nossos slides, o exercício não utiliza divisão por $m$ no cálculo da função custo (nem para a perda softmax nem para a regularização).\n",
    "\n",
    "Alguns exemplos de diferença de notação:\n",
    "- Slides (EEL7514) $\\longrightarrow$ Exercício\n",
    "- Número de amostras: $m \\longrightarrow N$\n",
    "- Número de atributos: $n \\longrightarrow D$\n",
    "\n",
    "Para exemplificar (e considerando que esse conteúdo inicial já foi coberto pelo Exercício 5), as soluções das duas primeiras tarefas são fornecidas abaixo:\n",
    "\n",
    "```python\n",
    "def affine_forward(x, w, b):\n",
    "    out = x.reshape(x.shape[0],-1) @ w + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    x, w, b = cache\n",
    "    dx = (dout @ w.T).reshape(x.shape)\n",
    "    dw = x.reshape(x.shape[0],-1).T @ dout\n",
    "    db = dout.sum(axis=0)\n",
    "    return dx, dw, db\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
